{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential , Model\n",
    "from keras.layers import Input , RepeatVector , TimeDistributed  , GRU , Bidirectional , LSTM , Dense\n",
    "from keras import regularizers\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.models import model_from_json\n",
    "from keras.datasets import mnist\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_model(maxlen , word_vector):\n",
    "    inputs = Input(shape=(maxlen , word_vector))\n",
    "    encoded = GRU(256 , return_sequences=True)(inputs)\n",
    "    encoded = GRU(128, return_sequences=False)(encoded)\n",
    "    decoded = RepeatVector(maxlen)(encoded)\n",
    "    decoded = GRU(128,return_sequences=True)(decoded)\n",
    "    decoded = GRU(word_vector ,return_sequences=True)(decoded)\n",
    "    output = TimeDistributed(Dense(word_vector))(decoded)\n",
    "    sequence_autoencoder = Model(inputs, output)\n",
    "    \n",
    "    encoder = Model(inputs, encoded)\n",
    "    return sequence_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(BASEPATH):\n",
    "    data_files = []\n",
    "    filenames = os.listdir(BASEPATH)[:20000]\n",
    "    for filename in tqdm(filenames):\n",
    "        with open(os.path.join(BASEPATH,filename) , 'rb') as f:\n",
    "            data_files.append(pickle.load(f))\n",
    "    return data_files\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(data_files, word_vector, func_len , vector_size ):\n",
    "    unknown = 'unknown'\n",
    "    zero_padding = [0] * vector_size\n",
    "    result_data = []\n",
    "    file = []\n",
    "    for data_file in tqdm(data_files):\n",
    "        for blocks in data_file:\n",
    "            vec_block = []\n",
    "            for block in blocks[:func_len]:\n",
    "                for mnemonic in block:\n",
    "                    try:\n",
    "                        vec_block.append(word_vector[mnemonic])\n",
    "                    except:\n",
    "                        vec_block.append(word_vector[unknown])\n",
    "            if (len(vec_block) >= 30):\n",
    "                if (len(vec_block) < func_len):\n",
    "                    for i in range(0, func_len - len(vec_block)):\n",
    "                        vec_block.append(zero_padding)\n",
    "                file.append(vec_block[:func_len])\n",
    "    X_train = np.array(file)\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec.load('word2vec_0402_16_upgrade.wv')\n",
    "word_vector = word2vec.wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10000/10000 [03:37<00:00, 45.94it/s]\n"
     ]
    }
   ],
   "source": [
    "data_files = data_load(r'C:\\capstone\\modeling\\3\\data\\ben')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10000/10000 [06:28<00:00, 25.76it/s]\n"
     ]
    }
   ],
   "source": [
    "pre_datas = pre_processing(data_files , word_vector , 100 ,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :  2071751\n",
      "Test :  230195\n"
     ]
    }
   ],
   "source": [
    "data_size = len(pre_datas)\n",
    "rate = 0.9\n",
    "X_train = pre_datas[:int(data_size*rate)]\n",
    "X_test = pre_datas[int(data_size*rate):]\n",
    "print('Train : ', len(X_train))\n",
    "print('Test : ', len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100, 16)           0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 100, 256)          209664    \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 128)               147840    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 100, 128)          98688     \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 100, 16)           6960      \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 100, 16)           272       \n",
      "=================================================================\n",
      "Total params: 463,424\n",
      "Trainable params: 463,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "double_model = double_model(100,16)\n",
    "double_model.compile(optimizer='adam' , loss='mae')\n",
    "double_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2071751, 100, 16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2071751 samples, validate on 230195 samples\n",
      "Epoch 1/40\n",
      " - 765s - loss: 0.8074 - val_loss: 0.8204\n",
      "Epoch 2/40\n",
      " - 764s - loss: 0.8160 - val_loss: 0.8154\n",
      "Epoch 3/40\n",
      " - 763s - loss: 0.8098 - val_loss: 0.7932\n",
      "Epoch 4/40\n",
      " - 763s - loss: 0.8083 - val_loss: 0.8009\n",
      "Epoch 5/40\n",
      " - 763s - loss: 0.8099 - val_loss: 0.7877\n",
      "Epoch 6/40\n",
      " - 713s - loss: 0.8062 - val_loss: 0.8284\n",
      "Epoch 7/40\n",
      " - 763s - loss: 0.8065 - val_loss: 0.8269\n",
      "Epoch 8/40\n",
      " - 764s - loss: 0.8053 - val_loss: 0.7732\n",
      "Epoch 9/40\n",
      " - 763s - loss: 0.7968 - val_loss: 0.8647\n",
      "Epoch 10/40\n",
      " - 763s - loss: 0.7987 - val_loss: 0.7725\n",
      "Epoch 11/40\n",
      " - 762s - loss: 0.7972 - val_loss: 0.7916\n",
      "Epoch 12/40\n",
      " - 762s - loss: 0.8006 - val_loss: 0.7843\n",
      "Epoch 13/40\n",
      " - 762s - loss: 0.7984 - val_loss: 0.7780\n",
      "Epoch 14/40\n",
      " - 767s - loss: 0.7942 - val_loss: 0.7719\n",
      "Epoch 15/40\n",
      " - 764s - loss: 0.7947 - val_loss: 0.8242\n",
      "Epoch 16/40\n",
      " - 765s - loss: 0.7932 - val_loss: 0.7971\n",
      "Epoch 17/40\n",
      " - 761s - loss: 0.8062 - val_loss: 0.7764\n",
      "Epoch 18/40\n",
      " - 763s - loss: 0.7888 - val_loss: 0.7828\n",
      "Epoch 19/40\n",
      " - 767s - loss: 0.7844 - val_loss: 0.7711\n",
      "Epoch 20/40\n",
      " - 768s - loss: 0.7858 - val_loss: 0.8130\n",
      "Epoch 21/40\n",
      " - 768s - loss: 0.7849 - val_loss: 0.7585\n",
      "Epoch 22/40\n",
      " - 767s - loss: 0.7842 - val_loss: 0.7783\n",
      "Epoch 23/40\n",
      " - 767s - loss: 0.7973 - val_loss: 0.7968\n",
      "Epoch 24/40\n",
      " - 768s - loss: 0.7942 - val_loss: 0.7857\n",
      "Epoch 25/40\n",
      " - 767s - loss: 0.7894 - val_loss: 0.7805\n",
      "Epoch 26/40\n",
      " - 767s - loss: 0.7816 - val_loss: 0.7673\n",
      "Epoch 27/40\n",
      " - 768s - loss: 0.7830 - val_loss: 0.7598\n",
      "Epoch 28/40\n",
      " - 768s - loss: 0.7832 - val_loss: 0.7619\n",
      "Epoch 29/40\n",
      " - 768s - loss: 0.7787 - val_loss: 0.7526\n",
      "Epoch 30/40\n",
      " - 767s - loss: 0.7742 - val_loss: 0.8156\n",
      "Epoch 31/40\n",
      " - 764s - loss: 0.7766 - val_loss: 0.7546\n",
      "Epoch 32/40\n",
      " - 765s - loss: 0.7755 - val_loss: 0.8347\n",
      "Epoch 33/40\n",
      " - 765s - loss: 0.7758 - val_loss: 0.7509\n",
      "Epoch 34/40\n",
      " - 761s - loss: 0.7776 - val_loss: 0.7538\n",
      "Epoch 35/40\n",
      " - 763s - loss: 0.7712 - val_loss: 0.8033\n",
      "Epoch 36/40\n",
      " - 764s - loss: 0.7812 - val_loss: 0.7792\n",
      "Epoch 37/40\n",
      " - 764s - loss: 0.7800 - val_loss: 0.7711\n",
      "Epoch 38/40\n",
      " - 765s - loss: 0.7781 - val_loss: 0.7697\n",
      "Epoch 39/40\n",
      " - 762s - loss: 0.7844 - val_loss: 0.8035\n",
      "Epoch 40/40\n",
      " - 761s - loss: 0.7786 - val_loss: 0.7495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2334d0b6b08>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_model.fit(X_train, X_train,epochs=40,batch_size=2048,shuffle=False,verbose =2 ,validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk0406\n"
     ]
    }
   ],
   "source": [
    "model_json = double_model.to_json()\n",
    "with open(\"model_ida_gru_0406.json\", \"w\") as json_file : \n",
    "    json_file.write(model_json)\n",
    "\n",
    "double_model.save_weights(\"model_ida_gru_0406.h5\")\n",
    "print(\"Saved model to disk0406\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
