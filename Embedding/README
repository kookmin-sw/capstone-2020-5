임베딩을 수행한 코드와 결과를 정리한 폴더입니다.
다음 알고리즘 수행 결과를 담고 있습니다.
<파일 5000개> Embedding.ipynb
1. Word2Vec (Skip-Gram)
2. Word2Vec (CBOW)
3. FastText (Skip-Gram)
4. FastText (CBOW)
------------------------
<파일 55000개> Word2vecSkipCbow.ipynb
1. Word2Vec (Skip-Gram) (VectorSize = 32 64 128 256) (Window = 2 3 4) (min=1000)
2. Word2Vec (CBOW) (VectorSize = 32 64 128 256) (Window = 2 3 4) (min=1000)
총 24개 모델
------------------------
1. 파생되는 뉴모닉을 합침ex: mov.ovf , push.ovf 등등  mov push 로 합침
2. 5000개 미만으로 등장하는 뉴모닉은 ‘unknown’ 으로 바꿈 (임베딩 학습 파일 10만개 기준)
3. Padding 이라는 단어도 임베딩을 시킴

위 3가지 조건을 통해 자주 등장하는 단어는 임베딩 , 처음보는 뉴모닉 임베딩 , 패딩 임베딩 모든 경우 임베딩이 


![Embbeding](https://user-images.githubusercontent.com/28583535/82005337-146f6580-96a0-11ea-8f30-e6a33a0653c4.PNG)
