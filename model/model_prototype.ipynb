{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "capstone-renew.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObcJl2aufML9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 구글 드라이브 마운트, 주피터에선 사용 X\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsvBQ2KRfQnA",
        "colab_type": "code",
        "outputId": "353b5bcf-991b-4420-a145-655e55614997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 필수 라이브러리 import\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense, LSTM\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1KjSUDMfSU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 변수 설정\n",
        "normal_path = '/content/gdrive/My Drive/캡스톤/pre_processing_data2/' #정상파일 경로\n",
        "mal_path = '/content/gdrive/My Drive/캡스톤/mal_pre/' #악성파일 경로\n",
        "model_path = '/content/gdrive/My Drive/캡스톤/' #모델 가중치 저장 경로\n",
        "max_len = 3 #n-그램 설정\n",
        "batch_size = 1024\n",
        "epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyWVTwnofcyn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 함수 설정 ###\n",
        "\n",
        "# 데이터 불러오기\n",
        "def data_load(path):\n",
        "  file_names = os.listdir(path)\n",
        "  data = []\n",
        "\n",
        "  for file_name in file_names:\n",
        "    f = open(path + file_name)\n",
        "    tokens = f.read()\n",
        "    data.append(tokens.split('\\n'))\n",
        "  \n",
        "  return data\n",
        "\n",
        "# 데이터 전처리\n",
        "## word -> index 변환\n",
        "def get_index(word, t):\n",
        "  if word in t.word_index:\n",
        "    return [t.word_index[word]]\n",
        "  else:\n",
        "    return [0]\n",
        "\n",
        "## 시퀀스 데이터 생성\n",
        "def seqs_gen(data, t):\n",
        "  seqs = []\n",
        "  zero_vec = [0] * (max_len - 2)\n",
        "  for words in data:\n",
        "    seqs.append(zero_vec + get_index(words[0], t) + get_index(words[1], t))\n",
        "    for i in range(2, len(words)):\n",
        "      seqs.append(seqs[i - 2][1:] + get_index(words[i], t))\n",
        "  return np.array(seqs)\n",
        "\n",
        "## x와 y형태로 분할\n",
        "def slice_nparray(seqs):\n",
        "  x = seqs[:,:-1]\n",
        "  y = to_categorical(seqs[:,-1], num_classes=vocab_size)\n",
        "  \n",
        "  return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya1T2Nl6gPq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터 불러오기 및 train, valid 분할\n",
        "normal_data = data_load(normal_path)\n",
        "mal_data = data_load(mal_path)\n",
        "\n",
        "train = normal_data[:80]\n",
        "valid = normal_data[80:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2P3m3dcgW_g",
        "colab_type": "code",
        "outputId": "203b69cd-6ca9-4ff5-a8f5-ca0940aae6de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# word_index 생성\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(train)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "print('단어 집합의 크기 : %d' % vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합의 크기 : 593\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiGH66BujiUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 시퀀스 데이터 생성\n",
        "normal_train_seqs = seqs_gen(train, t)\n",
        "normal_valid_seqs = seqs_gen(valid, t)\n",
        "mal_seqs = seqs_gen(mal_data, t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHzGgWnUhcJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x와 y로 분할\n",
        "x_train, y_train = slice_nparray(normal_train_seqs)\n",
        "x_valid, y_valid = slice_nparray(normal_valid_seqs)\n",
        "x_test, y_test = slice_nparray(mal_seqs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqJpGQqmtALA",
        "colab_type": "code",
        "outputId": "0fdd6f37-4287-4718-ad06-9d0fe7f40760",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# 모델 정의\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=max_len-1))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(vocab_size, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YriNqids9ze",
        "colab_type": "code",
        "outputId": "e1af4b58-31e0-4c0b-da8f-061ac8f0c535",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        }
      },
      "source": [
        "# 모델 최초 생성\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=epochs,\n",
        "          batch_size=batch_size,\n",
        "          verbose=1)\n",
        "model.save_weights(model_path + \"lstm_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "2696067/2696067 [==============================] - 38s 14us/step - loss: 3.6089 - acc: 0.1678\n",
            "Epoch 2/10\n",
            "2696067/2696067 [==============================] - 29s 11us/step - loss: 3.5450 - acc: 0.1754\n",
            "Epoch 3/10\n",
            "2696067/2696067 [==============================] - 29s 11us/step - loss: 3.5440 - acc: 0.1759\n",
            "Epoch 4/10\n",
            "2696067/2696067 [==============================] - 29s 11us/step - loss: 3.5431 - acc: 0.1765\n",
            "Epoch 5/10\n",
            "2696067/2696067 [==============================] - 29s 11us/step - loss: 3.5423 - acc: 0.1767\n",
            "Epoch 6/10\n",
            "2696067/2696067 [==============================] - 29s 11us/step - loss: 3.5417 - acc: 0.1768\n",
            "Epoch 7/10\n",
            "2696067/2696067 [==============================] - 29s 11us/step - loss: 3.5411 - acc: 0.1769\n",
            "Epoch 8/10\n",
            "2696067/2696067 [==============================] - 29s 11us/step - loss: 3.5406 - acc: 0.1770\n",
            "Epoch 9/10\n",
            "2696067/2696067 [==============================] - 29s 11us/step - loss: 3.5402 - acc: 0.1771\n",
            "Epoch 10/10\n",
            "2696067/2696067 [==============================] - 29s 11us/step - loss: 3.5399 - acc: 0.1770\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5bSmJuM194J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모델 가중치 불러오기\n",
        "model.load_weights(model_path + 'lstm_model.h5')\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aROfNLa2CCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모델 평가 함수\n",
        "def evaluate(x, y, dataset):\n",
        "  pred_y = model.predict(x)\n",
        "  valid_len = len(pred_y)\n",
        "\n",
        "  threshold = round(vocab_size * 0.9)\n",
        "  count = 0\n",
        "  well = 0\n",
        "  index = 0\n",
        "  abnormal = 0\n",
        "  abnormals = []\n",
        "\n",
        "  for i in range(valid_len):\n",
        "    print(\"\\r{} / {}\".format(i + 1, valid_len), end=\"\")\n",
        "    valid_index = np.where(y[i] == 1)[0]\n",
        "\n",
        "    if valid_index == 0:\n",
        "      pred_per = 0\n",
        "    else:\n",
        "      pred_per = pred_y[i][valid_index]\n",
        "      pred_y[i].sort()\n",
        "\n",
        "    if pred_per >= pred_y[i][threshold]:\n",
        "      well += 1\n",
        "\n",
        "    count += 1\n",
        "    if count == len(dataset[index]):\n",
        "      abnormal = count - well\n",
        "      score = abnormal / len(dataset[index])\n",
        "      index += 1\n",
        "      count = 0\n",
        "      well = 0\n",
        "      abnormals.append(score)\n",
        "  print()\n",
        "  abnormals = np.array(abnormals)\n",
        "  print(\"악성의 비율: {}\".format(abnormals.mean()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVBoktP3BIm0",
        "colab_type": "code",
        "outputId": "66ee4fc6-e233-4657-8d95-beee623d0076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# 모델 평가\n",
        "evaluate(x_train, y_train, normal_data[:80])\n",
        "evaluate(x_valid, y_valid, normal_data[80:])\n",
        "evaluate(x_test, y_test, mal_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2696067 / 2696067\n",
            "악성의 비율: 0.05003652134733598\n",
            "677198 / 677198\n",
            "악성의 비율: 0.06196408747917996\n",
            "518339 / 518339\n",
            "악성의 비율: 0.13876476357540685\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}